{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## 2. Split Text\n",
    "\n",
    "The clean data contains a little over 150,000 phrase pairs and some of the pairs toward the end of the file are very long.\n",
    "\n",
    "The complexity of the model increases with the number of examples, length of phrases, and size of the vocabulary.\n",
    "\n",
    "We will simplify the problem by reducing the dataset to the first 50,000 examples in the file; these will be the shortest phrases in the dataset.\n",
    "\n",
    "Further, we will then stake the first 45,000 of those as examples for training and the remaining 5,000 examples to test the fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(fname):\n",
    "    return load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, fname):\n",
    "    dump(sentences, open(fname, 'wb'))\n",
    "    print('Saved: %s' % fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['im helping you', 'ich helfe dir'],\n",
       "       ['it wasnt ours', 'es war nicht unseres'],\n",
       "       ['tom walked into the bar', 'tom ging in die kneipe'],\n",
       "       ['he has an eye for art', 'er hat einen blick fur kunst'],\n",
       "       ['do you use aftershave', 'benutzen sie aftershave'],\n",
       "       ['i said drop your weapon',\n",
       "        'ich sagte lassen sie die waffe fallen'],\n",
       "       ['ill stop by later', 'ich komme spater vorbei'],\n",
       "       ['do you happen to know tom', 'kennen sie zufalligerweise tom'],\n",
       "       ['he sketched an apple', 'er zeichnete einen apfel'],\n",
       "       ['tom has ocd', 'tom leidet an einer zwangserkrankung']],\n",
       "      dtype='<U370')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['i truly loved her', 'ich liebte sie wirklich'],\n",
       "       ['are you a good dancer', 'sind sie ein guter tanzer'],\n",
       "       ['do you know that person', 'kennst du diese person'],\n",
       "       ['im over eighteen', 'ich bin uber'],\n",
       "       ['whats wrong', 'was ist das problem'],\n",
       "       ['i decided to try again',\n",
       "        'ich habe beschlossen es noch einmal zu versuchen'],\n",
       "       ['tom is unmerciful', 'tom ist umbarmherzig'],\n",
       "       ['are you writing a letter', 'schreibst du einen brief'],\n",
       "       ['that was loud', 'das war laut'],\n",
       "       ['when can i visit you', 'wann kann ich euch besuchen']],\n",
       "      dtype='<U370')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the english-german-both.pkl that contains all of the train and test examples that we can use to define the parameters of the problem, such as max phrase lengths and the vocabulary, and the english-german-train.pkl and english-german-test.pkl files for the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
